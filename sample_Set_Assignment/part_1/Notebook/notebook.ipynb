{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ml_main\\New folder\\Assignment\\source_code\\myenv\\lib\\site-packages\\weaviate\\__init__.py:144: DeprecationWarning: Dep010: Importing AuthApiKey from weaviate is deprecated. Import AuthApiKey from its module: weaviate.auth\n",
      "  _Warnings.root_module_import(name, map_[name])\n",
      "C:\\Users\\aks\\AppData\\Local\\Temp\\ipykernel_24796\\1674973894.py:12: DeprecationWarning: \n",
      "Python client v3 `weaviate.Client(...)` connections and methods are deprecated and will\n",
      "            be removed by 2024-11-30.\n",
      "\n",
      "            Upgrade your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "                - For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "                - For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "\n",
      "            If you have to use v3 code, install the v3 client and pin the v3 dependency in your requirements file: `weaviate-client>=3.26.7;<4.0.0`\n",
      "  client = weaviate.Client(\n",
      "d:\\ml_main\\New folder\\Assignment\\source_code\\myenv\\lib\\site-packages\\weaviate\\warnings.py:162: DeprecationWarning: Dep016: Python client v3 `weaviate.Client(...)` connections and methods are deprecated and will\n",
      "            be removed by 2024-11-30.\n",
      "\n",
      "            Upgrade your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "                - For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "                - For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "\n",
      "            If you have to use v3 code, install the v3 client and pin the v3 dependency in your requirements file: `weaviate-client>=3.26.7;<4.0.0`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WEAVIATE_URL = os.getenv(\"WEAVIATE_CLUSTER\")\n",
    "WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url = WEAVIATE_URL , auth_client_secret = weaviate.AuthApiKey(WEAVIATE_API_KEY) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ml_main\\New folder\\Assignment\\source_code\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "embedding_model_name = \"Sentence-Transformers/all-mpnet-base-v2\"\n",
    "\n",
    "\n",
    "# model_kwargs = {\"device\" : \"cuda\"}\n",
    "\n",
    "embeddings  = HuggingFaceBgeEmbeddings(\n",
    "    model_name = embedding_model_name ,\n",
    "    # model_kwargs = model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pdf\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "path = 'Assignment.pdf'\n",
    "\n",
    "loader = PyPDFLoader(path , extract_images = True)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Assignment.pdf', 'page': 0}, page_content='Gen\\nAI\\nEngineer\\n/\\nMachine\\nLearning\\nEngineer\\nAssignment\\nPart\\n1:\\nRetrieval-Augmented\\nGeneration\\n(RAG)\\nModel\\nfor\\nQA\\nBot\\nProblem\\nStatement:\\nDevelop\\na\\nRetrieval-Augmented\\nGeneration\\n(RAG)\\nmodel\\nfor\\na\\nQuestion\\nAnswering\\n(QA)\\nbot\\nfor\\na\\nbusiness.\\nUse\\na\\nvector\\ndatabase\\nlike\\nPinecone\\nDB\\nand\\na\\ngenerative\\nmodel\\nlike\\nCohere\\nAPI\\n(or\\nany\\nother\\navailable\\nalternative).\\nThe\\nQA\\nbot\\nshould\\nbe\\nable\\nto\\nretrieve\\nrelevant\\ninformation\\nfrom\\na\\ndataset\\nand\\ngenerate\\ncoherent\\nanswers.\\nTask\\nRequirements:\\n1.\\nImplement\\na\\nRAG-based\\nmodel\\nthat\\ncan\\nhandle\\nquestions\\nrelated\\nto\\na\\nprovided\\ndocument\\nor\\ndataset.\\n2.\\nUse\\na\\nvector\\ndatabase\\n(such\\nas\\nPinecone\\n)\\nto\\nstore\\nand\\nretrieve\\ndocument\\nembeddings\\nefficiently.\\n3.\\nTest\\nthe\\nmodel\\nwith\\nseveral\\nqueries\\nand\\nshow\\nhow\\nwell\\nit\\nretrieves\\nand\\ngenerates\\naccurate\\nanswers\\nfrom\\nthe\\ndocument.\\nDeliverables:\\n●\\nA\\nColab\\nnotebook\\ndemonstrating\\nthe\\nentire\\npipeline,\\nfrom\\ndata\\nloading\\nto\\nquestion\\nanswering.\\n●\\nDocumentation\\nexplaining\\nthe\\nmodel\\narchitecture,\\napproach\\nto\\nretrieval,\\nand\\nhow\\ngenerative\\nresponses\\nare\\ncreated.\\n●\\nProvide\\nseveral\\nexample\\nqueries\\nand\\nthe\\ncorresponding\\noutputs.\\n{S\\nAMPLE\\nET'),\n",
       " Document(metadata={'source': 'Assignment.pdf', 'page': 1}, page_content=\"Part\\n2:\\nInteractive\\nQA\\nBot\\nInterface\\nProblem\\nStatement:\\nDevelop\\nan\\ninteractive\\ninterface\\nfor\\nthe\\nQA\\nbot\\nfrom\\nPart\\n1,\\nallowing\\nusers\\nto\\ninput\\nqueries\\nand\\nretrieve\\nanswers\\nin\\nreal\\ntime.\\nThe\\ninterface\\nshould\\nenable\\nusers\\nto\\nupload\\ndocuments\\nand\\nask\\nquestions\\nbased\\non\\nthe\\ncontent\\nof\\nthe\\nuploaded\\ndocument.\\nTask\\nRequirements:\\n1.\\nBuild\\na\\nsimple\\nfrontend\\ninterface\\nusing\\nStreamlit\\nor\\nGradio\\n,\\nallowing\\nusers\\nto\\nupload\\nPDF\\ndocuments\\nand\\nask\\nquestions.\\n2.\\nIntegrate\\nthe\\nbackend\\nfrom\\nPart\\n1\\nto\\nprocess\\nthe\\nPDF,\\nstore\\ndocument\\nembeddings,\\nand\\nprovide\\nreal-time\\nanswers\\nto\\nuser\\nqueries.\\n3.\\nEnsure\\nthat\\nthe\\nsystem\\ncan\\nhandle\\nmultiple\\nqueries\\nefficiently\\nand\\nprovide\\naccurate,\\ncontextually\\nrelevant\\nresponses.\\n4.\\nAllow\\nusers\\nto\\nsee\\nthe\\nretrieved\\ndocument\\nsegments\\nalongside\\nthe\\ngenerated\\nanswer.\\nDeliverables:\\n●\\nA\\ndeployed\\nQA\\nbot\\nwith\\na\\nfrontend\\ninterface\\nwhere\\nusers\\ncan\\nupload\\ndocuments\\nand\\ninteract\\nwith\\nthe\\nbot.\\n●\\nDocumentation\\non\\nhow\\nthe\\nuser\\ncan\\nupload\\nfiles,\\nask\\nquestions,\\nand\\nview\\nthe\\nbot's\\nresponses.\\n●\\nExample\\ninteractions\\ndemonstrating\\nthe\\nbot's\\ncapabilities.\\nGuidelines:\\n●\\nUse\\nDocker\\nto\\ncontainerize\\nthe\\napplication\\nfor\\neasy\\ndeployment.\\n●\\nEnsure\\nthe\\nsystem\\ncan\\nhandle\\nlarge\\ndocuments\\nand\\nmultiple\\nqueries\\nwithout\\nsignificant\\nperformance\\ndrops.\\n●\\nShare\\nthe\\ncode,\\ndeployment\\ninstructions,\\nand\\nthe\\nfinal\\nworking\\nmodel\\nthrough\\nGitHub.\\nGeneral\\nGuidelines:\\n1.\\nEnsure\\nmodular\\nand\\nscalable\\ncode\\nfollowing\\nbest\\npractices\\nfor\\nboth\\nfrontend\\nand\\nbackend\\ndevelopment.\\n2.\\nDocument\\nyour\\napproach\\nthoroughly,\\nexplaining\\nyour\\ndecisions,\\nchallenges\\nfaced,\\nand\\nsolutions.\\n3.\\nProvide\\na\\ndetailed\\nReadMe\\nfile\\nin\\nyour\\nGitHub\\nrepository,\\nincluding\\nsetup\\nand\\nusage\\ninstructions.\\n4.\\nSubmissions\\nshould\\ninclude:\\n○\\nSource\\ncode\\nfor\\nboth\\nthe\\nnotebook\\nand\\nthe\\ninterface.\\n○\\nA\\nfully\\nfunctional\\nColab\\nnotebook.\\n○\\nDocumentation\\non\\nthe\\npipeline\\nand\\ndeployment\\ninstructions.\")]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000 , chunk_overlap= 20)\n",
    "\n",
    "docs = text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Assignment.pdf', 'page': 0}, page_content='Gen\\nAI\\nEngineer\\n/\\nMachine\\nLearning\\nEngineer\\nAssignment\\nPart\\n1:\\nRetrieval-Augmented\\nGeneration\\n(RAG)\\nModel\\nfor\\nQA\\nBot\\nProblem\\nStatement:\\nDevelop\\na\\nRetrieval-Augmented\\nGeneration\\n(RAG)\\nmodel\\nfor\\na\\nQuestion\\nAnswering\\n(QA)\\nbot\\nfor\\na\\nbusiness.\\nUse\\na\\nvector\\ndatabase\\nlike\\nPinecone\\nDB\\nand\\na\\ngenerative\\nmodel\\nlike\\nCohere\\nAPI\\n(or\\nany\\nother\\navailable\\nalternative).\\nThe\\nQA\\nbot\\nshould\\nbe\\nable\\nto\\nretrieve\\nrelevant\\ninformation\\nfrom\\na\\ndataset\\nand\\ngenerate\\ncoherent\\nanswers.\\nTask\\nRequirements:\\n1.\\nImplement\\na\\nRAG-based\\nmodel\\nthat\\ncan\\nhandle\\nquestions\\nrelated\\nto\\na\\nprovided\\ndocument\\nor\\ndataset.\\n2.\\nUse\\na\\nvector\\ndatabase\\n(such\\nas\\nPinecone\\n)\\nto\\nstore\\nand\\nretrieve\\ndocument\\nembeddings\\nefficiently.\\n3.\\nTest\\nthe\\nmodel\\nwith\\nseveral\\nqueries\\nand\\nshow\\nhow\\nwell\\nit\\nretrieves\\nand\\ngenerates\\naccurate\\nanswers\\nfrom\\nthe\\ndocument.\\nDeliverables:\\n●\\nA\\nColab\\nnotebook\\ndemonstrating\\nthe\\nentire\\npipeline,\\nfrom\\ndata\\nloading\\nto\\nquestion\\nanswering.\\n●\\nDocumentation\\nexplaining\\nthe\\nmodel\\narchitecture,\\napproach\\nto\\nretrieval,\\nand\\nhow\\ngenerative'),\n",
       " Document(metadata={'source': 'Assignment.pdf', 'page': 0}, page_content='and\\nhow\\ngenerative\\nresponses\\nare\\ncreated.\\n●\\nProvide\\nseveral\\nexample\\nqueries\\nand\\nthe\\ncorresponding\\noutputs.\\n{S\\nAMPLE\\nET'),\n",
       " Document(metadata={'source': 'Assignment.pdf', 'page': 1}, page_content=\"Part\\n2:\\nInteractive\\nQA\\nBot\\nInterface\\nProblem\\nStatement:\\nDevelop\\nan\\ninteractive\\ninterface\\nfor\\nthe\\nQA\\nbot\\nfrom\\nPart\\n1,\\nallowing\\nusers\\nto\\ninput\\nqueries\\nand\\nretrieve\\nanswers\\nin\\nreal\\ntime.\\nThe\\ninterface\\nshould\\nenable\\nusers\\nto\\nupload\\ndocuments\\nand\\nask\\nquestions\\nbased\\non\\nthe\\ncontent\\nof\\nthe\\nuploaded\\ndocument.\\nTask\\nRequirements:\\n1.\\nBuild\\na\\nsimple\\nfrontend\\ninterface\\nusing\\nStreamlit\\nor\\nGradio\\n,\\nallowing\\nusers\\nto\\nupload\\nPDF\\ndocuments\\nand\\nask\\nquestions.\\n2.\\nIntegrate\\nthe\\nbackend\\nfrom\\nPart\\n1\\nto\\nprocess\\nthe\\nPDF,\\nstore\\ndocument\\nembeddings,\\nand\\nprovide\\nreal-time\\nanswers\\nto\\nuser\\nqueries.\\n3.\\nEnsure\\nthat\\nthe\\nsystem\\ncan\\nhandle\\nmultiple\\nqueries\\nefficiently\\nand\\nprovide\\naccurate,\\ncontextually\\nrelevant\\nresponses.\\n4.\\nAllow\\nusers\\nto\\nsee\\nthe\\nretrieved\\ndocument\\nsegments\\nalongside\\nthe\\ngenerated\\nanswer.\\nDeliverables:\\n●\\nA\\ndeployed\\nQA\\nbot\\nwith\\na\\nfrontend\\ninterface\\nwhere\\nusers\\ncan\\nupload\\ndocuments\\nand\\ninteract\\nwith\\nthe\\nbot.\\n●\\nDocumentation\\non\\nhow\\nthe\\nuser\\ncan\\nupload\\nfiles,\\nask\\nquestions,\\nand\\nview\\nthe\\nbot's\\nresponses.\\n●\"),\n",
       " Document(metadata={'source': 'Assignment.pdf', 'page': 1}, page_content=\"bot's\\nresponses.\\n●\\nExample\\ninteractions\\ndemonstrating\\nthe\\nbot's\\ncapabilities.\\nGuidelines:\\n●\\nUse\\nDocker\\nto\\ncontainerize\\nthe\\napplication\\nfor\\neasy\\ndeployment.\\n●\\nEnsure\\nthe\\nsystem\\ncan\\nhandle\\nlarge\\ndocuments\\nand\\nmultiple\\nqueries\\nwithout\\nsignificant\\nperformance\\ndrops.\\n●\\nShare\\nthe\\ncode,\\ndeployment\\ninstructions,\\nand\\nthe\\nfinal\\nworking\\nmodel\\nthrough\\nGitHub.\\nGeneral\\nGuidelines:\\n1.\\nEnsure\\nmodular\\nand\\nscalable\\ncode\\nfollowing\\nbest\\npractices\\nfor\\nboth\\nfrontend\\nand\\nbackend\\ndevelopment.\\n2.\\nDocument\\nyour\\napproach\\nthoroughly,\\nexplaining\\nyour\\ndecisions,\\nchallenges\\nfaced,\\nand\\nsolutions.\\n3.\\nProvide\\na\\ndetailed\\nReadMe\\nfile\\nin\\nyour\\nGitHub\\nrepository,\\nincluding\\nsetup\\nand\\nusage\\ninstructions.\\n4.\\nSubmissions\\nshould\\ninclude:\\n○\\nSource\\ncode\\nfor\\nboth\\nthe\\nnotebook\\nand\\nthe\\ninterface.\\n○\\nA\\nfully\\nfunctional\\nColab\\nnotebook.\\n○\\nDocumentation\\non\\nthe\\npipeline\\nand\\ndeployment\\ninstructions.\")]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Weaviate.from_documents(\n",
    "    docs , embeddings , client = client , by_text= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen\n",
      "AI\n",
      "Engineer\n",
      "/\n",
      "Machine\n",
      "Learning\n",
      "Engineer\n",
      "Assignment\n",
      "Part\n",
      "1:\n",
      "Retrieval-Augmented\n",
      "Generation\n",
      "(RAG)\n",
      "Model\n",
      "for\n",
      "QA\n",
      "Bot\n",
      "Problem\n",
      "Statement:\n",
      "Develop\n",
      "a\n",
      "Retrieval-Augmented\n",
      "Generation\n",
      "(RAG)\n",
      "model\n",
      "for\n",
      "a\n",
      "Question\n",
      "Answering\n",
      "(QA)\n",
      "bot\n",
      "for\n",
      "a\n",
      "business.\n",
      "Use\n",
      "a\n",
      "vector\n",
      "database\n",
      "like\n",
      "Pinecone\n",
      "DB\n",
      "and\n",
      "a\n",
      "generative\n",
      "model\n",
      "like\n",
      "Cohere\n",
      "API\n",
      "(or\n",
      "any\n",
      "other\n",
      "available\n",
      "alternative).\n",
      "The\n",
      "QA\n",
      "bot\n",
      "should\n",
      "be\n",
      "able\n",
      "to\n",
      "retrieve\n",
      "relevant\n",
      "information\n",
      "from\n",
      "a\n",
      "dataset\n",
      "and\n",
      "generate\n",
      "coherent\n",
      "answers.\n",
      "Task\n",
      "Requirements:\n",
      "1.\n",
      "Implement\n",
      "a\n",
      "RAG-based\n",
      "model\n",
      "that\n",
      "can\n",
      "handle\n",
      "questions\n",
      "related\n",
      "to\n",
      "a\n",
      "provided\n",
      "document\n",
      "or\n",
      "dataset.\n",
      "2.\n",
      "Use\n",
      "a\n",
      "vector\n",
      "database\n",
      "(such\n",
      "as\n",
      "Pinecone\n",
      ")\n",
      "to\n",
      "store\n",
      "and\n",
      "retrieve\n",
      "document\n",
      "embeddings\n",
      "efficiently.\n",
      "3.\n",
      "Test\n",
      "the\n",
      "model\n",
      "with\n",
      "several\n",
      "queries\n",
      "and\n",
      "show\n",
      "how\n",
      "well\n",
      "it\n",
      "retrieves\n",
      "and\n",
      "generates\n",
      "accurate\n",
      "answers\n",
      "from\n",
      "the\n",
      "document.\n",
      "Deliverables:\n",
      "●\n",
      "A\n",
      "Colab\n",
      "notebook\n",
      "demonstrating\n",
      "the\n",
      "entire\n",
      "pipeline,\n",
      "from\n",
      "data\n",
      "loading\n",
      "to\n",
      "question\n",
      "answering.\n",
      "●\n",
      "Documentation\n",
      "explaining\n",
      "the\n",
      "model\n",
      "architecture,\n",
      "approach\n",
      "to\n",
      "retrieval,\n",
      "and\n",
      "how\n",
      "generative\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    vector_db.similarity_search(\n",
    "        \"what is Pinecone?\" , k =3\n",
    ")[0].page_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation using LLM and retrived context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistent for question-answering task .\n",
    "Use the following piece of retrived context to answer the question .\n",
    "If you don't know ,just say idk.\n",
    "Use 10 sentence at maximux and keep the answer concise.\n",
    "Don't repeat the context again and again in Answer\n",
    "Question : {question}\n",
    "Context : {context}\n",
    "Answer : \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing LLM from hugging face\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "huggingfacehub_api_token = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "    repo_id = 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "    model_kwargs={'temperature' : 1 , \"max_length\" : 180}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "\n",
    "    {'context' :retriever  , 'question' : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "     |output_parser\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rag_chain.invoke(\"What is Pinecone?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer : \n",
      "\n",
      "Pinecone is a vector database that is used to store and retrieve document embeddings efficiently. It is a type of database that is used to store and retrieve data in a vector space. In the context of the QA bot, Pinecone is used to store and retrieve document embeddings, which are used to generate coherent answers to user queries. The QA bot is designed to be able to retrieve relevant information from a dataset and generate coherent answers. The model architecture\n"
     ]
    }
   ],
   "source": [
    "print(output[4032:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[3946:].find('Answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninstructions.\")]\n",
      "Answer : \n",
      "\n",
      "The overall assignment work involves developing a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA) bot problem statement. The model should be able to handle questions related to the provided document or dataset and retrieve relevant information from it. The model should also generate coherent answers.\n",
      "\n",
      "The task requirements include implementing an RAG-based model, using a vector database like Pinecone DB to store and retrieve document embeddings efficiently, and\n"
     ]
    }
   ],
   "source": [
    "output = rag_chain.invoke(\"what is the overall assignment work\")\n",
    "print(output[4032:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "You are an assistent for question-answering task .\n",
      "Use the following piece of retrived context to answer the question .\n",
      "If you don't know ,just say idk.\n",
      "Use 10 sentence at maximux and keep the answer concise.\n",
      "Don't repeat the context again and again in Answer\n",
      "Question : what is the overall assignment work\n",
      "Context : [Document(metadata={'page': 0, 'source': 'Assignment.pdf'}, page_content='Gen\\nAI\\nEngineer\\n/\\nMachine\\nLearning\\nEngineer\\nAssignment\\nPart\\n1:\\nRetrieval-Augmented\\nGeneration\\n(RAG)\\nModel\\nfor\\nQA\\nBot\\nProblem\\nStatement:\\nDevelop\\na\\nRetrieval-Augmented\\nGeneration\\n(RAG)\\nmodel\\nfor\\na\\nQuestion\\nAnswering\\n(QA)\\nbot\\nfor\\na\\nbusiness.\\nUse\\na\\nvector\\ndatabase\\nlike\\nPinecone\\nDB\\nand\\na\\ngenerative\\nmodel\\nlike\\nCohere\\nAPI\\n(or\\nany\\nother\\navailable\\nalternative).\\nThe\\nQA\\nbot\\nshould\\nbe\\nable\\nto\\nretrieve\\nrelevant\\ninformation\\nfrom\\na\\ndataset\\nand\\ngenerate\\ncoherent\\nanswers.\\nTask\\nRequirements:\\n1.\\nImplement\\na\\nRAG-based\\nmodel\\nthat\\ncan\\nhandle\\nquestions\\nrelated\\nto\\na\\nprovided\\ndocument\\nor\\ndataset.\\n2.\\nUse\\na\\nvector\\ndatabase\\n(such\\nas\\nPinecone\\n)\\nto\\nstore\\nand\\nretrieve\\ndocument\\nembeddings\\nefficiently.\\n3.\\nTest\\nthe\\nmodel\\nwith\\nseveral\\nqueries\\nand\\nshow\\nhow\\nwell\\nit\\nretrieves\\nand\\ngenerates\\naccurate\\nanswers\\nfrom\\nthe\\ndocument.\\nDeliverables:\\n●\\nA\\nColab\\nnotebook\\ndemonstrating\\nthe\\nentire\\npipeline,\\nfrom\\ndata\\nloading\\nto\\nquestion\\nanswering.\\n●\\nDocumentation\\nexplaining\\nthe\\nmodel\\narchitecture,\\napproach\\nto\\nretrieval,\\nand\\nhow\\ngenerative'), Document(metadata={'page': 1, 'source': 'Assignment.pdf'}, page_content=\"Part\\n2:\\nInteractive\\nQA\\nBot\\nInterface\\nProblem\\nStatement:\\nDevelop\\nan\\ninteractive\\ninterface\\nfor\\nthe\\nQA\\nbot\\nfrom\\nPart\\n1,\\nallowing\\nusers\\nto\\ninput\\nqueries\\nand\\nretrieve\\nanswers\\nin\\nreal\\ntime.\\nThe\\ninterface\\nshould\\nenable\\nusers\\nto\\nupload\\ndocuments\\nand\\nask\\nquestions\\nbased\\non\\nthe\\ncontent\\nof\\nthe\\nuploaded\\ndocument.\\nTask\\nRequirements:\\n1.\\nBuild\\na\\nsimple\\nfrontend\\ninterface\\nusing\\nStreamlit\\nor\\nGradio\\n,\\nallowing\\nusers\\nto\\nupload\\nPDF\\ndocuments\\nand\\nask\\nquestions.\\n2.\\nIntegrate\\nthe\\nbackend\\nfrom\\nPart\\n1\\nto\\nprocess\\nthe\\nPDF,\\nstore\\ndocument\\nembeddings,\\nand\\nprovide\\nreal-time\\nanswers\\nto\\nuser\\nqueries.\\n3.\\nEnsure\\nthat\\nthe\\nsystem\\ncan\\nhandle\\nmultiple\\nqueries\\nefficiently\\nand\\nprovide\\naccurate,\\ncontextually\\nrelevant\\nresponses.\\n4.\\nAllow\\nusers\\nto\\nsee\\nthe\\nretrieved\\ndocument\\nsegments\\nalongside\\nthe\\ngenerated\\nanswer.\\nDeliverables:\\n●\\nA\\ndeployed\\nQA\\nbot\\nwith\\na\\nfrontend\\ninterface\\nwhere\\nusers\\ncan\\nupload\\ndocuments\\nand\\ninteract\\nwith\\nthe\\nbot.\\n●\\nDocumentation\\non\\nhow\\nthe\\nuser\\ncan\\nupload\\nfiles,\\nask\\nquestions,\\nand\\nview\\nthe\\nbot's\\nresponses.\\n●\"), Document(metadata={'page': 0, 'source': 'Assignment.pdf'}, page_content='and\\nhow\\ngenerative\\nresponses\\nare\\ncreated.\\n●\\nProvide\\nseveral\\nexample\\nqueries\\nand\\nthe\\ncorresponding\\noutputs.\\n{S\\nAMPLE\\nET'), Document(metadata={'page': 1, 'source': 'Assignment.pdf'}, page_content=\"bot's\\nresponses.\\n●\\nExample\\ninteractions\\ndemonstrating\\nthe\\nbot's\\ncapabilities.\\nGuidelines:\\n●\\nUse\\nDocker\\nto\\ncontainerize\\nthe\\napplication\\nfor\\neasy\\ndeployment.\\n●\\nEnsure\\nthe\\nsystem\\ncan\\nhandle\\nlarge\\ndocuments\\nand\\nmultiple\\nqueries\\nwithout\\nsignificant\\nperformance\\ndrops.\\n●\\nShare\\nthe\\ncode,\\ndeployment\\ninstructions,\\nand\\nthe\\nfinal\\nworking\\nmodel\\nthrough\\nGitHub.\\nGeneral\\nGuidelines:\\n1.\\nEnsure\\nmodular\\nand\\nscalable\\ncode\\nfollowing\\nbest\\npractices\\nfor\\nboth\\nfrontend\\nand\\nbackend\\ndevelopment.\\n2.\\nDocument\\nyour\\napproach\\nthoroughly,\\nexplaining\\nyour\\ndecisions,\\nchallenges\\nfaced,\\nand\\nsolutions.\\n3.\\nProvide\\na\\ndetailed\\nReadMe\\nfile\\nin\\nyour\\nGitHub\\nrepository,\\nincluding\\nsetup\\nand\\nusage\\ninstructions.\\n4.\\nSubmissions\\nshould\\ninclude:\\n○\\nSource\\ncode\\nfor\\nboth\\nthe\\nnotebook\\nand\\nthe\\ninterface.\\n○\\nA\\nfully\\nfunctional\\nColab\\nnotebook.\\n○\\nDocumentation\\non\\nthe\\npipeline\\nand\\ndeployment\\ninstructions.\")]\n",
      "Answer : \n",
      "\n",
      "The overall assignment work involves developing a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA) bot problem statement. The model should be able to handle questions related to the provided document or dataset and retrieve relevant information from it. The model should also generate coherent answers.\n",
      "\n",
      "The task requirements include implementing an RAG-based model, using a vector database like Pinecone DB to store and retrieve document embeddings efficiently, and\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent\\ninstructions.\")]\n",
      "Answer : \n",
      "\n",
      "There are two parts in the assignment.\n"
     ]
    }
   ],
   "source": [
    "output = rag_chain.invoke(\"How many parts are there is assignment?\")\n",
    "print(output[4032:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yment\\ninstructions.\")]\n",
      "Answer : \n",
      "\n",
      "The second part of the assignment is to develop an interactive QA bot interface that allows users to input queries and retrieve answers in real-time. The interface should enable users to upload PDF documents and ask questions based on the content of the uploaded document. The system should be able to handle multiple queries efficiently and provide accurate, contextually relevant responses. The deliverables for this part of the assignment include a deployed QA bot with a frontend interface, documentation on how users can upload files,\n"
     ]
    }
   ],
   "source": [
    "output = rag_chain.invoke(\"what's the second part of the assignment?\")\n",
    "print(output[4032:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
